{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "# Week 3 - Convolutional neural networks\n",
    "(*Material also available p. 287 - 321 in Bishop*)\n",
    "\n",
    "Convolutional Neural Networks (CNN's) are the most common application of convolutions. Here, the weights bewteen neurons usually learned in a fully connected neural network are replaced by kernels, which are learned instead.\n",
    "\n",
    "At the end of this week, you should be able to:\n",
    "\n",
    "- Explain how padding works to perserve image dimensions during convolution\n",
    "\n",
    "---\n",
    "\n",
    "CNN's are most commonly used today for three distinct purposes:\n",
    "\n",
    "1. Image classification: The amount of parameters in a CNN is effectively *invariant* with regards to the size of the input image. Additionally, CNN's basically learn to do feature extraction to find important parts of an image in much the same way regular convolution does.\n",
    "2. Signal analysis: Much like when dealing with images, the number of parameters in a CNN does not scale with the length of a signal used for input. Also again, convolving a signal is basically the same as applying a filter, which can be used to extract useful features. This makes CNN's important for cases such as denoising or classifying signals of varying length.\n",
    "3. Noise prediction in diffusion models: Probably the most \"modern\" application, a variant of CNN's, a \"U-net\" is used for many diffusion models that serve to generate for example images.\n",
    "\n",
    "---\n",
    "\n",
    "Convolutional layers in CNN's function much the same way as regular convolutions. One of the key differences is that there are typically multiple kernels that all convolve the same image\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/alright_gif.gif\" alt=\"\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Like every other class of neural network out there, CNN's have a million-and-one specific\n",
    "methods and ideas to improve their performance, in these exercises, we only really explain pooling. Also, there are a billion-and-two different specific architectures (like the aforementioned \"U-net\"), that are useful in specific or different cases. For this course, we will only examine a specific implementaiton of \"basic\" CNN: VGG16-D. Even so, understanding the fundamentals of convolutional layers, you should be able to grasp the basics of most other implementation of CNN that exists.\n",
    "\n",
    "---\n",
    "\n",
    "**Remember to ask questions as much as you need, includin the \"stupid\" ones. CNN's, perhaps more than most other subjects, has tons of \"I really should understand this, but I don't, therefore I must be dumb\"-moments. Don't sweat it if it seems trivial, *it probably isn't*!**\n",
    "\n",
    "---\n",
    "\n",
    "Many of these exercises, will require writing and changing some PyTorch syntax. This is not required material for the exam, but is pretty much necessary to learn at some point. Don't get stuck on the specifics of syntax you don't understand, but at the same time, don't completely neglect learning this syntax! \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.functional\n",
    "from torchvision.transforms import ToTensor\n",
    "import PIL.Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Finally, a useful function that we do not explicitly use, but you can check out if you want\n",
    "# from torchsummary import summary # courtesy of https://stackoverflow.com/questions/55875279/how-to-get-an-output-dimension-for-each-layer-of-the-neural-network-in-pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "## Part 1 - Manual calculations for CNN\n",
    "\n",
    "These intitial exercises will focus on the parameters and dimensionality of input and output in CNN's. If at any point you feel confident in these, you can skip to **part 2** for details on how to implement a CNN?\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 1 - Parameters in learnable kernels\n",
    "\n",
    "For this exercise, we consider a kernel in a convolutional layer with the following parameters:\n",
    "\n",
    "- Size: $3 \\times 3$\n",
    "- Padding (zero-padded): $1$\n",
    "- Stride: $1$\n",
    "- Input channels: $3$\n",
    "\n",
    "#### **1.1. How many learnable parameters does this kernel have in a CNN?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **1.2. In general, how many parameters does an ùëõ \\times m kernel with d input channels, ùëù padding and ùë† stride have?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 2 - Parameters in conolutional layers\n",
    "\n",
    "\n",
    "Say you now have a convolutional layer with the following hyperparameters:\n",
    "\n",
    "- Kernel size: $n \\times m$\n",
    "- Input channels: $d$\n",
    "- Output channels: $k$\n",
    "\n",
    "\n",
    "#### **2.1: How many learnable parameters does this whole convolutional layer have?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **2.2. üíª Check your manual calculation against that of the torch implementation below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for a toy convolutional layer\n",
    "in_channels = 3\n",
    "out_channels = 128\n",
    "kernel_size = (2,3) # Does not have to be a tuple, if int, will be converted to a square kernel\n",
    "stride = 1\n",
    "padding = 0\n",
    "bias = True\n",
    "\n",
    "# Define conv layer and sum number of parameters\n",
    "conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "num_params = sum(p.numel() for p in conv_layer.parameters())\n",
    "\n",
    "# Calculate number of parameters manually\n",
    "raise NotImplementedError(\"REMEMBER TO CALCULATE THE PARAMETERS MANUALLY!\")\n",
    "num_params_manual = ...\n",
    "\n",
    "print(f\"Torch thinks there are {num_params} parameters in the layer layer \\nCompared to {num_params_manual} calculated manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 3 - Dimensions after convolutional layers\n",
    "\n",
    "\n",
    "We are now given a convolutional layer with the following hyperparameters:\n",
    "- Size: $n \\times n$\n",
    "- Padding: $p$\n",
    "- Stride: $s$\n",
    "- Input channels: $d$\n",
    "- Output channels: $k$\n",
    "\n",
    "And an input image with dimensions $N\\times N \\times d$, so a square image with $d$ channels\n",
    "\n",
    "\n",
    "#### **3.1. What is the dimensionality of the output image after passing through the given convolutional layer?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **3.2. Likewise, what is the dimensionality after a *pooling* layer with the same parameters as the convolutional layer?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **3.3. üíª Check your answer against the torch code below**\n",
    "\n",
    "\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once more, Parameters for a simple convolutional layer\n",
    "in_channels = 3\n",
    "out_channels = 128\n",
    "kernel_size = (2,3) # Does not have to be a tuple, if int, will be converted to a square kernel\n",
    "stride = 1\n",
    "padding = 0\n",
    "bias = True\n",
    "\n",
    "# Define conv layer and sum number of parameters\n",
    "conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "\n",
    "# Define two dimensional max pooling layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "# Load image and convert to tensor\n",
    "a_cool_image = PIL.Image.open(\"images/a_cool_image.jpg\").convert(\"RGB\")\n",
    "# Unsqueeze to add batch dimension (because torch is nasty like that and expects a batch dimension)\n",
    "a_cool_image_tensor = torchvision.transforms.functional.to_tensor(a_cool_image).unsqueeze(0)\n",
    "\n",
    "print(f\"Image shape before convolution: {a_cool_image_tensor.shape}\")\n",
    "\n",
    "# Apply convolution and max pooling\n",
    "convolved_image = conv_layer(a_cool_image_tensor)\n",
    "pooled_image = max_pool_layer(a_cool_image_tensor)\n",
    "\n",
    "print(f\"Image shape after convolution: {convolved_image.shape}\")\n",
    "print(f\"Image shape after max pooling: {pooled_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 4 - Benefits of channels\n",
    "\n",
    "\n",
    "It should be clear now, that each channel in the output image, corresponds to one kernel\n",
    "being run across all channels in the input image. \n",
    "\n",
    "Therefore with $k$ output channels, we have to train $k$ kernels, each with $m \\times n \\times d + 1$ parameters.\n",
    "\n",
    "\n",
    "#### **4.1. Discuss some of the benefits of having a larger number of output channels compared to input channels.**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.2. Increasing the number of output channels in one layer obviously leads to more kernels (and therefore more parameters), but how does this increased number of output channels affect the kernels in subsequent layers in a neural network?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 5 - (Not) Flipping the kernel\n",
    "\n",
    "If you read the torch documentation of [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), you won‚Äôt find anywhere that it flips the kernel as is otherwise required to go from cross-correlation to convolution.\n",
    "\n",
    "**5.1. Explain how PyTorch, a respectable deep learning framework, can get away with *not*\n",
    "flipping the kernel**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 2 - Coding CNN's\n",
    "\n",
    "Theres a **bunch** of boilerplate that goes into every NN implementation in Python. Our advice, is to understand and solve the exercises given first, and then look into this boilerplate.\n",
    "\n",
    "A lot of people and guides tell you to simply ignore this boilerplate code, and you can... **But it will haunt you later**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### 1. Import packages, check cuda availability, and seed everything\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you have cuda available, and use if you do\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Set a random seed for everything important\n",
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a seed with a random integer, in this case, I choose my verymost favourite sequence of numbers\n",
    "seed_everything(sum([115, 107, 105, 98, 105, 100, 105, 32, 116, 111, 105, 108, 101, 116]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### 2. Download and define either [Cifar10]() or Mnist as dataset\n",
    "\n",
    "We are going to use the [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html), that is a bunch of images of 10 different classes. cifar100 with 100 classes also exists, but models for that would be a bit too large for this course...\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset, change 'dataset' to 'cifar10' if you want to use CIFAR-10 instead\n",
    "dataset = 'mnist'\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "    # Purely for our convenience - Mapping from cifar labels to human readable classes\n",
    "    cifar10_classes = {\n",
    "        0: 'airplane',\n",
    "        1: 'automobile',\n",
    "        2: 'bird',\n",
    "        3: 'cat',\n",
    "        4: 'deer',\n",
    "        5: 'dog',\n",
    "        6: 'frog',\n",
    "        7: 'horse',\n",
    "        8: 'ship',\n",
    "        9: 'truck'\n",
    "    }\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_set = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "    test_set = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "print(f\"There are {len(train_set)} examples in the training set\")\n",
    "print(f\"There are {len(test_set)} examples in the test set \\n\")\n",
    "\n",
    "print(f\"Image shape is: {train_set[0][0].shape}, label example is {train_set[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### 3. Define collate function and dataloaders\n",
    "\n",
    "Dataloders are not *necessary* per say, but are very useful. \n",
    "\n",
    "A dataloader is a torch utility that functions like an *iterator*, that is, we can loop over it where it gives batches of data and labels. The number of data points in each batch is defined by the *batch_size*. *Shuffle ensures we shuffle the ordering of images so the model does not get any inherent information from the ordering of data. \n",
    "\n",
    "The collate function controls how each batch is transformed before being given when we iterate over the dataloader. In this case, we simple just the *default_collate* (which we normally don't need to specify), but also cast each tensor to our chosen device, either CUDA or CPU.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function is called on each batch, the dataloader yields\n",
    "# Here, we modify it to also cast the tensor to our desired device\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "def collate_fn(batch):\n",
    "    return tuple(x_.to(device) for x_ in default_collate(batch))\n",
    "\n",
    "# Why don't we shuffle the test set? Because we don't need to.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False, collate_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### 4. Show a few examples from the dataset\n",
    "\n",
    "As is tradition when making CNN introductions\n",
    "\n",
    "The quality is pretty terrible. You can reason a bit about why...\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of 10 different classes in the dataset, then get values\n",
    "sampled_indices = [np.random.choice(np.where(np.array(train_set.targets) == i)[0]) for i in range(10)]\n",
    "examples = train_set.data[sampled_indices]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot examples\n",
    "for i in range(len(examples)):\n",
    "    img = examples[i]\n",
    "    if dataset == 'mnist':\n",
    "        img = img.squeeze(0)  # Remove the channel dimension for MNIST\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Class {i}')\n",
    "    else:\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Class {i}:{cifar10_classes[train_set.targets[sampled_indices[i]]]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Define a CNN class\n",
    "\n",
    "We have now reached the point, where we can define the main class that'll compose our CNN.\n",
    "\n",
    "Keep in mind the following:\n",
    "- The amount of parameters in a convolutional layer is $(n * m * l + 1) * k$  \n",
    "   - Where $n, m$ is the kernel size $(x, y)$, $l$ is the input channels, and $k$ is the $\\text{output channels} + 1$, is beacuse of a bias term that is done for each input\n",
    "  - Basically, $n * m * l$ corresponds to kernels mapping to values in another \"image\"\n",
    "  - Each kernel has a bias term unique to it. Each kernel produces one \"image\", that are then stacked on top of each other, for a total of $k$ images.\n",
    "  - CONV2D does NOT flip the kernel!!!\n",
    "    - But, it doesn't really mattter, since it is just learned anyways...\n",
    "\n",
    "Finally, we found an answer on if it is better to ReLU first or Max Pool first... it *mostly* doesn't matter\n",
    "- [But it is slightly better to maxpool first, max-pooling and monotonely increasing non-linearities commute. This means that MaxPool(Relu(x)) = Relu(MaxPool(x)) for any input](https://stackoverflow.com/questions/35543428/activation-function-after-pooling-layer-or-convolutional-layer\n",
    ")\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### $\\star$ Exercise 5: Coding the CNN\n",
    "\n",
    "As the star indicates, this is an optional exercise. You are encouraged to search online, ask you friends, LLM, or the TA's on tips or best practices to implement a working CNN. This part is optional because it *is* mostly coding, but it is recommended, as it will probably help to know *how* to do it, both genrally, and with the first assignment.\n",
    "\n",
    "#### **üíª 5.1. Complete the layers for the CNN class. There are a few ways you *could* go about doing this:**\n",
    "\n",
    "**1. Do it with a [torch.nn.sequential](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html) object, ala what they do [here](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html). The pros here are that it may be easier, and it greatly simplifies your forward function**\n",
    "\n",
    "**2. Do it by creating a series of layers as function objects. In your `__init__` function have `self.linear_layer_1 = ...`, `self.conv_layer_1 = ...`, so on, and so forth. Your forward function will then need to pass input through all of these in succession. Pros here, are it gives you some more flexibility and functionality than nn.sequential, but requires a bit more code.**\n",
    "\n",
    "**3. Do it entirely in the forward function by using torch's functional module, torch.nn.functional. Here, you need to keep track of the parameters of each kernel and layer, through class objects, for example `self.layer_1_weights = nn.Parameter(...)`, `self.layer_1_bias = nn.Parameter(...)`, so on and so forth. Cons are obvious, it takes a hell of a lot more work, and we don't recommend doing it, but the pros are it gives you very explicit control over the initilization of the weights.**\n",
    "\n",
    "#### **üíª 5.2. Write the training loop of the CNN class. It should do the following:**\n",
    "\n",
    "**1. For each epoch in how many epochs we train...**\n",
    "   1. **For each input in that given epoch...**\n",
    "      1. **Calculate the estimated probabiltiies of each class by passing the input through the layers**\n",
    "      2. **Compare these probabilities to the given targets to calculate a loss**\n",
    "      3. **Perform backpropagation on this loss**\n",
    "      4. **Take a step with the optimizer**\n",
    "      5. **Zero the optimizer's gradient**\n",
    "   2. **(Somewhat optional) Keep track of how many predictions were correct in a given epoch, so we can get a running accuracy across each epoch.**\n",
    "\n",
    "**If you need *local* inspiration, you can look at how the eval function is already defined...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=64*6*6, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        # The only part we keep here, is the .to(device), as we wanna cast each of our layer objects...\n",
    "        # to our chosen device (either CUDA or CPU)\n",
    "        self.layers = (...).to(device)\n",
    "                \n",
    "        # Loss function and optimizer, as you know, Adam is the meta\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass using the model\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Tensor to be passed through the model's layers. Shape should fit with the initial layer of the model.\n",
    "\n",
    "        Returns: torch.tensor of the result of passing x through the layers of the model\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def train(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        Args:\n",
    "            train_dataloader (iterable): Some iterable that yields tuples of (inputs, targets)\n",
    "            ... each inputs should of shape (b, c, n, m) where b is the batch size, c is the number of channels, and n, m are the input image dimensions\n",
    "            ... each targets should be of shape (b) with integers corresponding to the index of the correct class\n",
    "            ... all values yielded from the train_dataloader should be torch tensors\n",
    "            epochs (int): An integer representing how many epochs (total runthroughs of the data) the model should train for. Defaults to 1.\n",
    "            val_dataloader (Optional, iterable): An iterable of the same type as train_dataloader. If set, the model will test itself after each train epoch\n",
    "\n",
    "        Returns:\n",
    "            train_accs, val_accs (list[float], list[float]): Lists containing floats of the training- and validation-accuracy respectively after each epoch.\n",
    "        \"\"\"\n",
    "        \n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                # Code goes here...\n",
    "                raise NotImplementedError('REMEMBER TO MAKE YOUR TRAINING LOOP!')\n",
    "\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "                \n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If we have val dataloader, we can evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                acc = self.eval(val_dataloader)\n",
    "                val_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "        \n",
    "        return train_accs, val_accs\n",
    "\n",
    "    def eval(self, test_dataloader):\n",
    "        \n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            # Get predictions\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            # Remember, outs are probabilities (so there's 10 for each input)\n",
    "            # The classification the network wants to assign, must therefore be the probability with the larget value\n",
    "            # We find that using argmax (dim=1, because dim=0 would be across batch dimension)\n",
    "            classifications = torch.argmax(logits, dim=1)\n",
    "            total_acc += (classifications == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        if not os.path.exists(\"saved_models\"):\n",
    "            os.makedirs(\"saved_models\")\n",
    "        save_path = os.path.join(\"saved_models\", model_name)\n",
    "        torch.save(self.state_dict(), os.path.join(\"saved_models\", model_name))\n",
    "        print(f\"Model successfully saved at {save_path}\")\n",
    "    \n",
    "    def load_model(self, model_name):\n",
    "        model_path = os.path.join(\"saved_models/\", model_name)\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Model state_dict successfully loaded from {model_path}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### A small hack\n",
    "\n",
    "Normally, you have to ask ChatGPT, a friend or the dark Gods to find out what the dimensionality of the data is before the first linear layer (why has Torch not implemented this automatically????) that is, *finding out the number of input \"neurons\" after each convolutional layer, before the first linear layer*.\n",
    "\n",
    "Anyways, the below function solves that, while still being a bit cursed\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dim_before_first_linear(layers, in_dim, in_channels, brain=False):\n",
    "    \"\"\"\n",
    "    Calculate the dimensions before the first linear layer.\n",
    "    DISCLAIMER: THIS IS IN NO WAY GUARANTEED TO WORK, AND IS PURELY FOR CONVENIENCE!\n",
    "    If it fails, you will have to manually calculate the dimensions before the first linear layer\n",
    "\n",
    "    Args:\n",
    "        layers: Sequential layers to analyze\n",
    "        in_dim: Input dimension (assumed square)\n",
    "        in_channels: Number of input channels\n",
    "        brain: If True, returns (height, width, channels) tuple. If False, returns total features.\n",
    "    \n",
    "    Returns:\n",
    "        If brain=True: (height, width, channels) tuple\n",
    "        If brain=False: total number of features (height * width * channels)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        current_dim = in_dim\n",
    "        current_channels = in_channels\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.MaxPool2d):\n",
    "                # If the layer padding is same we do not need to change the dimension of the input...\n",
    "                if layer.padding == 'same':\n",
    "                    if isinstance(layer, nn.Conv2d):\n",
    "                        current_channels = layer.out_channels\n",
    "                    # For MaxPool2d with padding='same', we need to calculate the output size\n",
    "                    if isinstance(layer, nn.MaxPool2d):\n",
    "                        # For padding='same', output size = ceil(input_size / stride)\n",
    "                        stride = layer.stride if isinstance(layer.stride, int) else layer.stride[0]\n",
    "                        current_dim = (current_dim + stride - 1) // stride\n",
    "                    continue\n",
    "                vals = {\n",
    "                    'kernel_size': layer.kernel_size if isinstance(layer.kernel_size, int) else layer.kernel_size[0],\n",
    "                    'stride': layer.stride if isinstance(layer.stride, int) else layer.stride[0],\n",
    "                    'padding': layer.padding if isinstance(layer.padding, int) else layer.padding[0],\n",
    "                    'dilation': layer.dilation if isinstance(layer.dilation, int) else layer.dilation[0]\n",
    "                }\n",
    "                current_dim = (current_dim + 2*vals['padding'] - vals['dilation']*(vals['kernel_size'])) // vals['stride'] + 1\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                current_channels = layer.out_channels\n",
    "        \n",
    "            if isinstance(layer, nn.Linear):\n",
    "                if brain:\n",
    "                    return current_dim, current_channels\n",
    "                else:\n",
    "                    return current_dim * current_dim * current_channels\n",
    "        \n",
    "        # If no linear layer found, return the final dimensions\n",
    "        if brain:\n",
    "            return current_dim, current_channels\n",
    "        else:\n",
    "            return current_dim * current_dim * current_channels\n",
    "    \n",
    "    except:\n",
    "        print(\"\"\"\n",
    "                Ooops! Something went wrong in getting the dimension of the data before the first linear layer!\n",
    "                If you'll refer to the docstring of this function, you should see the disclaimer, that it may very well fail (it really is a hack)\n",
    "                In this case, you should instead manually calculate the dimensionality of the linear layer, immediately following the last convolutional layer\n",
    "                Good luck!\n",
    "                \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN\n",
    "\n",
    "*Putting it all together, we should be able to train the CNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "\n",
    "# Make a dummy model to find out the size before the first linear layer\n",
    "CNN_model = CNN(num_classes=10, in_channels=in_channels, lr=0.001)\n",
    "feats_fore_linear = get_dim_before_first_linear(CNN_model.layers, in_width_height, in_channels, brain=False)\n",
    "\n",
    "# Now make true model when we know how many features we have before the first linear layer\n",
    "CNN_model = CNN(num_classes=10, in_channels=in_channels, features_fore_linear=feats_fore_linear, lr=0.001) \n",
    "\n",
    "train_epochs = 1\n",
    "train_accs, test_accs = CNN_model.train(train_dataloader, epochs=train_epochs,  val_dataloader=test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and test accuracies\n",
    "plt.plot(range(train_epochs), train_accs, label='Train')\n",
    "plt.plot(range(train_epochs), test_accs, label='Test')\n",
    "\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy over epochs, CNN model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 6: Considering the CNN's results\n",
    "\n",
    "\n",
    "#### **6.1. Reason a bit about why the training and test accuracy look as they do? One question could be: Why is the training accuracy lower than the test accuracy for the first epoch?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **6.2. üíª Make the network also save the average losses over each epoch as it trains. What does the training loss after each epoch tell you about the state of its training?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **6.3. Can you use the training loss as a measure for the performance of your model?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **6.4. üíª Try changing some of the hyperparameters of the network (layers, epochs, learning rate, etc.), are you able to achieve a better test accuracy?**\n",
    "\n",
    "#### **6.5. üíª Look into the torch documentatation for [saving and loading models](https://pytorch.org/tutorials/beginner/saving_loading_models.html), try to complete the functions for saving and loading your model resepectively**\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Defining a FeedForward Neural Network (FFNN)\n",
    "\n",
    "To have something to compare to our shiny new CNN, it makes sense to implement an FFNN, even though it really isn't meant for image data, it may still perform somewhat.\n",
    "\n",
    "Here, we don't need to reinvent the wheel, and a lot of boilerplate code is shared between our CNN and FFNN, so we write our FFNN as a *subclass* of the CNN model.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(CNN):\n",
    "    def __init__(self, in_features, num_classes, lr=0.001):\n",
    "        # Here, we call the init function of the \"grandparent\", as we do not necessarily want to call the\n",
    "        # init function of the CNN (which we would with super().__init__()) (as we don't have channels and whatnot), but torch still requires that we call\n",
    "        # __init__() for each class that subclasses from torch.nn.module\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=num_classes)\n",
    "        ).to(device)\n",
    "\n",
    "        # Despite only having linear layers, we still define our loss on the model's ability to find the right class (probabilities)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=lr)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # We flatten images, as we need to treat them as vectors\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # Super unecessary, but wanted to show that you can call the parent class' forward function\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Num in_channels really shouldn't be above 1, the FFNN probably can't handle that well\n",
    "num_in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "\n",
    "FFNN_model = FFNN(in_features=num_in_channels*in_width_height**2,num_classes=10, lr=0.001)\n",
    "\n",
    "train_epochs = 2\n",
    "train_accs, test_accs = FFNN_model.train(train_dataloader, epochs=train_epochs, val_dataloader=test_dataloader)\n",
    "\n",
    "# plot train and test accuracies\n",
    "plt.plot(range(train_epochs), train_accs, label='Train')\n",
    "plt.plot(range(train_epochs), test_accs, label='Test')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy over epochs, FFNN model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 7: Considering the FFNN's results\n",
    "\n",
    "#### **7.1. Test the FFNN model for both the CIFAR10 and the MNIST dataset. There should be a *huge* difference in test accuracy when using CIFAR10 compared to MNIST. Explain this gap, and why this gap is comparatively smaller for the CNN model**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **7.2. It should be obvious, that the FFNN model is inferior in terms of test accuracy for this particular task... Does it have any advantages over the CNN?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **7.3. üíª Try changing the layer structure of your FFNN to see if you can achieve a similar or better test accuracy on CIFAR10 than the CNN. Alternatively, try to see how much you need to hamstring the layers of the CNN to pull it down to the performance level of the FFNN.**\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 8: Examining the output of the convolutional layers layers\n",
    "\n",
    "CNN's obviously perform convolutions. We can somewhat examine the form of these convolutions to reason more about what the model \"looks for\".\n",
    "\n",
    "The code below grabs the intermediate output after each layer through torch's \"forward hoooks\", which are functions that can be attached to perform operations during a forward pass. We can try to plot the output after each forward layer to reason about what kind of feature extraction the CNN does.\n",
    "\n",
    "#### **8.1. Consider the intermediary kernel outputs of both a trained and an untrained CNN model, how do they differ?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **8.2. Another question?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = True\n",
    "\n",
    "if trained_model is False:\n",
    "    # Initialize a dummy untrained CNN model to compare activations with\n",
    "    in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "    in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "    feats_fore_linear = get_dim_before_first_linear(CNN_model.layers, in_width_height, in_channels, brain=False)\n",
    "    model = CNN(num_classes=10, in_channels=in_channels, features_fore_linear=feats_fore_linear, lr=0.001) \n",
    "\n",
    "else:\n",
    "    # Otherwise use a trained model\n",
    "    model = CNN_model\n",
    "\n",
    "# Dict to hold network activations\n",
    "activations = {}\n",
    "\n",
    "# Hook function, is a type of wrapper function that is then called whenever layer registered with it is run\n",
    "def forward_hook(layer_name, capture_dict):\n",
    "    def hook(module, input, output):\n",
    "        capture_dict[layer_name] = output\n",
    "    return hook\n",
    "\n",
    "# Register hooks for all Conv2d layers model\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        layer.register_forward_hook(forward_hook(name, activations))\n",
    "\n",
    "# Get just a single example from the test dataloader\n",
    "input_image, input_label = next(iter(test_dataloader))\n",
    "input_image, input_label = input_image[0].unsqueeze(0).to('cpu'), input_label[0].unsqueeze(0).to('cpu').numpy()[0]\n",
    "\n",
    "# Plot the image before convolutions\n",
    "if dataset == 'mnist':\n",
    "    plt.imshow(input_image.squeeze().numpy(), cmap='gray')\n",
    "    plt.title(f'Random image, label: {input_label}')\n",
    "\n",
    "elif dataset == 'cifar10':\n",
    "    plt.imshow(input_image.squeeze().transpose(0, 2).transpose(0, 1).numpy())\n",
    "    plt.title(f'Random image, label: {cifar10_classes[input_label]}')\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "# Forward pass - Use torch.no_grad() to not store gradients, since we are only interested in activations\n",
    "with torch.no_grad():\n",
    "    print(input_image.shape)\n",
    "    output = model(input_image.to(device))\n",
    "\n",
    "# Display the captured activations\n",
    "for layer_name, activation in activations.items():\n",
    "    print(f\"Layer: {layer_name}, Activation Shape: {activation.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(len(activations.items()), 4, figsize=(12, 6))\n",
    "\n",
    "for i, (layer_name, activation) in enumerate(activations.items()):\n",
    "    for j in range(4):\n",
    "        axes[i, j].imshow(activation[0, j, ...].to('cpu'), cmap='gray')\n",
    "        axes[i, j].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signals-and-data-autumn-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
